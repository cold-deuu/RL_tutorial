# RL Tutorial Repository

This repository contains sample codes for various policy gradient algorithms using the OpenAI Gym environment.

---

Base code : https://github.com/pasus/Reinforcement-Learning-Book-Revision



## Dependency
- Pytorch
- Open AI Gym
- Pygame
- Numpy
- Matplotlib

## How to Run?
### Train
```
cd rl_colddeuu/a2c
python3 a2c_main.py
```

### Run
```
cd rl_colddeuu/a2c
python3 a2c_agent_test.py
```

## To Do List
- [x] A2C
- [ ] A3C
- [ ] PPO

---

# A2C
Actor Critic Algorithm

## Algorithm

Train the Actor and Critic networks simultaneously.

- Actor Network : A policy network that outputs the mean and standard deviation of the policy's probability distribution.
- Critic Network : A value network that estimates the state value function and is used to compute the loss for training the actor.

## Concept

ì •ì±…ì„ $\theta$ ë¡œ íŒŒë¼ë¯¸í„°í™” í•œ ë„¤íŠ¸ì›Œí¬ë¡œ ì •ì˜í•œë‹¤. ì´ë•Œ ìµœì ì˜ $\theta$ ë¥¼ ì°¾ê¸° ìœ„í•´ì„œ Loss Function ì„ ì •ì˜í•˜ê³ , ì´ë¥¼ ìµœì í™” í•˜ê¸° ìœ„í•œ í•™ìŠµì„ ì§„í–‰í•œë‹¤.

ì´ë•Œ Loss Functionì€ Policy ì™€ Action Value(í–‰ë™ê°€ì¹˜, Q)ë¡œ êµ¬ì„±ëœë‹¤. ê·¸ëŸ¬ë‚˜ ì´ Loss Functionì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ í–‰ë™ê°€ì¹˜ì˜ ë³€í™”ì— Loss ê°€ ë§¤ìš° ë¯¼ê°í•´ì ¸ í•™ìŠµì— ë¶ˆì•ˆì •ì„±ì„ ì•¼ê¸°í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ Base Lineì„ ë¹¼ì£¼ì–´ í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì¸ë‹¤.

ì´ë•Œ Base Line ì€ ë³´í†µ State Value ë¥¼ ì‚¬ìš©í•˜ê³ , ì ì ˆí•œ ê·¼ì‚¬ë¥¼ ê±°ì¹˜ë©´ Policy ì˜ Loss Functionì´ State Value Function ê³¼ Policy ì˜ ì¡°í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.

ì´ë•Œ State Value Function ë˜í•œ $\phi$ ë¡œ íŒŒë¼ë¯¸í„°í™”í•œ ë„¤íŠ¸ì›Œí¬ë¡œ ì •ì˜í•˜ê³ , ì´ë¥¼ Critic Networkë¼ê³  í•˜ë©´, Critic Network ë¡œ State Value ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ Actor Network ë¥¼ í•™ìŠµí•˜ëŠ”ë° ì¬ì‚¬ìš©í•˜ë©° ìµœì ì˜ Policy Network ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤.

## Summary

### ğŸ“š Loss Functions
---
### ğŸ¯ Policy Gradient Loss (Without Baseline)

$$
\mathcal{L}_{\text{policy}} = - \mathbb{E}_{s,a} \left[ \log \pi_\theta(a|s) \cdot Q^\pi(s,a) \right]
$$

---

### ğŸ§˜â€â™‚ï¸ Policy Gradient Loss (With Baseline)

$\mathcal{L}_{\text{actor}} = - \mathbb{E}_{s,a} \left[ \log \pi_\theta(a|s) \cdot ( Q^\pi(s,a) - b(s) ) \right]$

â€» ë³´í†µ baseline b(s) ëŠ” ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ V(s) ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

---

### ğŸ’¡ Advantage-based Policy Loss

$A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$

$\mathcal{L}_{\text{actor}} = - \mathbb{E}_{s,a} \left[ \log \pi_\theta(a|s) \cdot A^\pi(s,a) \right]$

---

### ğŸ§  Critic Loss (State Value Function)

$\mathcal{L}_{\text{critic}} = \left( V_\phi(s) - R \right)^2$

$R = r + \gamma V_\phi(s')$
