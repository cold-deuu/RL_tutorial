# A2C
Actor Critic Algorithm


## ğŸŒ€ Rollout Demo

![Rollout Demo](assets/a2c_pendulum.gif)


## Algorithm

Train the Actor and Critic networks simultaneously.

- Actor Network : A policy network that outputs the mean and standard deviation of the policy's probability distribution.
- Critic Network : A value network that estimates the state value function and is used to compute the loss for training the actor.

## Concept

ì •ì±…ì„ $\theta$ ë¡œ íŒŒë¼ë¯¸í„°í™” í•œ ë„¤íŠ¸ì›Œí¬ë¡œ ì •ì˜í•œë‹¤. ì´ë•Œ ìµœì ì˜ $\theta$ ë¥¼ ì°¾ê¸° ìœ„í•´ì„œ Loss Function ì„ ì •ì˜í•˜ê³ , ì´ë¥¼ ìµœì í™” í•˜ê¸° ìœ„í•œ í•™ìŠµì„ ì§„í–‰í•œë‹¤.

ì´ë•Œ Loss Functionì€ Policy ì™€ Action Value(í–‰ë™ê°€ì¹˜, Q)ë¡œ êµ¬ì„±ëœë‹¤. ê·¸ëŸ¬ë‚˜ ì´ Loss Functionì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ í–‰ë™ê°€ì¹˜ì˜ ë³€í™”ì— Loss ê°€ ë§¤ìš° ë¯¼ê°í•´ì ¸ í•™ìŠµì— ë¶ˆì•ˆì •ì„±ì„ ì•¼ê¸°í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ Base Lineì„ ë¹¼ì£¼ì–´ í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì¸ë‹¤.

ì´ë•Œ Base Line ì€ ë³´í†µ State Value ë¥¼ ì‚¬ìš©í•˜ê³ , ì ì ˆí•œ ê·¼ì‚¬ë¥¼ ê±°ì¹˜ë©´ Policy ì˜ Loss Functionì´ State Value Function ê³¼ Policy ì˜ ì¡°í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.

ì´ë•Œ State Value Function ë˜í•œ $\phi$ ë¡œ íŒŒë¼ë¯¸í„°í™”í•œ ë„¤íŠ¸ì›Œí¬ë¡œ ì •ì˜í•˜ê³ , ì´ë¥¼ Critic Networkë¼ê³  í•˜ë©´, Critic Network ë¡œ State Value ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ Actor Network ë¥¼ í•™ìŠµí•˜ëŠ”ë° ì¬ì‚¬ìš©í•˜ë©° ìµœì ì˜ Policy Network ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤.

## How to Run?
### Train
```
cd rl_colddeuu/a2c
python3 a2c_main.py
```

### Run
```
cd rl_colddeuu/a2c
python3 a2c_agent_test.py
```

### Plot

```
Run reward_plot.mlx in MATLAB
```
